<html><HEAD>
<LINK REL=StyleSheet HREF="../style.css" TYPE="text/css">
</HEAD>
<body>

<p class="Path">
Prev: <a href="c_performance.htm">Testing and Evaluating</a>,
Next: <a href="c_nb_disc.htm">Naive Bayes with Discretization</a>,
Up: <a href="classification.htm">Classification</a>
</p>

<H1>Build Your Own Learner</H1>
<index name="classifiers/in Python">

<p>This part of tutorial will show how to build learners and
classifiers in Python, that is, how to build your own learners and
classifiers. Especially for those of you that want to test some of
your methods or want to combine existing techniques in Orange, this is
a very important topic. Developing your own learners in Python makes
prototyping of new methods fast and enjoyable.</p>

<p>There are different ways to build learners/classifiers in
Python. We will take the route that shows how to do this correctly, in
a sense that you will be able to use your learner as it would be any
learner that Orange originally provides. Distinct to Orange learners
is the way how they are invoked and what the return. Let us start with
an example. Say that we have a Learner(), which is some learner in
Orange. The learner can be called in two different ways:</p>

<xmp class="code">learner = Learner()
classifier = Learner(data)
</xmp>

<p>In the first line, the learner is invoked without the data set and
in that case it should return an instance of learner, such that later
you may say <code>classifier = learner(data)</code> or you may call
some validation procedure with a <code>learner</code> itself (say
<code>orngEval.CrossValidation([learner], data)</code>). In the second
line, learner is called with the data and returns a classifier.</p>

<p>Classifiers should be called with a data instance to classify,
and should return either a class value (by default), probability of
classes or both:</p>

<xmp class="code">value = classifier(instance)
value = classifier(instance, orange.GetValue)
probabilities = classifier(instance, orange.GetProbabilities)
value, probabilities = classifier(instance, orange.GetBoth)
</xmp>

<p>Here is a short example:</p>

<pre class="code">
> <strong>python</strong>
>>> <strong>import orange</strong>
>>> <strong>data = orange.ExampleTable("voting")</strong>
>>> <strong>learner = orange.BayesLearner()</strong>
>>> <strong>classifier = learner(data)</strong>
>>> <strong>classifier(data[0])</strong>
republican
>>> <strong>classifier(data[0], orange.GetBoth)</strong>
(republican, [0.99999994039535522, 7.9730767765795463e-008])
>>> <strong>classifier(data[0], orange.GetProbabilities)</strong>
[0.99999994039535522, 7.9730767765795463e-008]
>>> 
>>> <strong>c = orange.BayesLearner(data)</strong>
>>> <strong>c(data[12])</strong>
democrat
>>>
</pre>

<p>Throughout our examples, we will assume that our learner and the
corresponding classifier will be defined in a single file (module)
that will not contain any other code. This helps for code reuse, so
that if you want to use your new method anywhere else, you just import
it from that file. Each such module will contain a class
<code>Learner_Class</code> and a class <code>Classifier</code>.</p> We
will use this schema to define a learner that will use naive Bayesian classifier with embeded categorization of training data. Then we will show how to write naive Bayesian classifier in Python (that is, how to do this from
scratch). We conclude with Python implementation of bagging.</p>



<h2>Naive Bayes with Discretization</H2>

<p>Let us build a learner/classifier that is an extension of build-in
naive Bayes and which before learning categorizes the data (see also
the lesson on <a href= "o_categorization.htm">Categorization</a>). We
will define a module <a href="nbdisc.py">nbdisc.py</a> that will
implement two classes, Learner and Classifier. Following is a Python
code for a Learner class:</p>

<p class="header">function <code>Learner</code> from <a href=
"nbdisc.py">nbdisc.py</a></p>
<xmp class="code">class Learner(object):
    def __new__(cls, examples=None, name='discretized bayes', **kwds):
        learner = object.__new__(cls, **kwds)
        if examples:
            learner.__init__(name) # force init
            return learner(examples)
        else:
            return learner  # invokes the __init__

    def __init__(self, name='discretized bayes'):
        self.name = name

    def __call__(self, data, weight=None):
        disc = orange.Preprocessor_discretize( \
            data, method=orange.EntropyDiscretization())
        model = orange.BayesLearner(disc, weight)
        return Classifier(classifier = model)
</xmp>

<p>Learner_Class has three methods. Method <code>__new__</code>
creates the object and returns a learner or classifier, depending if
examples where passed to the call. If the examples were passed as an
argument than the method called the learner (invoking
<code>__call__</code> method). Method <code>__init__</code> is invoked
every time the class is called for the first time. Notice that all it
does is remembers the only argument that this class can be called
with, i.e. the argument <code>name</code> which defaults to
&lsquo;discretized bayes&rsquo;. If you would expect any other
arguments for your learners, you should handle them here (store them
as class&rsquo; attributes using the keyword <code>self</code>).</p>

<p>If we have created an instance of the learner (and did not pass the
examples as attributes), the next call of this learner will invoke a
method <code>__call__</code>, where the essence of our learner is
implemented. Notice also that we have included an attribute for vector
of instance weights, which is passed to naive Bayesian learner. In our
learner, we first discretize the data using Fayyad &amp; Irani&rsquo;s
entropy-based discretization, then build a naive Bayesian model and
finally pass it to a class <code>Classifier</code>. You may expect
that at its first invocation the <code>Classifier</code> will just
remember the model we have called it with:</p>

<p class="header">class Classifier from <a href=
"nbdisc.py">nbdisc.py</a></p>
<xmp class="code">class Classifier:
    def __init__(self, **kwds):
        self.__dict__.update(kwds)

    def __call__(self, example, resultType = orange.GetValue):
        return self.classifier(example, resultType)
</xmp>


<p>The method <code>__init__</code> in <code>Classifier</code> is
rather general: it makes <code>Classifier</code> remember all
arguments it was called with. They are then accessed through
<code>Classifiers</code>&rsquo; arguments
(<code>self.argument_name</code>). When Classifier is called, it
expects an example and an optional argument that specifies the type of
result to be returned.</p>

<p>This completes our code for naive Bayesian classifier with
discretization. You can see that the code is fairly short (fewer than 
20 lines), and it can be easily extended or changed if we want to do
something else as well (like feature subset selection, ...).</p>

<p>Here are now a few lines to test our code:</p>

<p class="header">uses <a href="iris.tab">iris.tab</a> and <a href=
"nbdisc.py">nbdisc.py</a></p>
<pre class="code">
> <code>python</code>
>>> <code>import orange, nbdisc</code>
>>> <code>data = orange.ExampleTable("iris")</code>
>>> <code>classifier = nbdisc.Learner(data)</code>
>>> <code>print classifier(data[100])</code>
Iris-virginica
>>> <code>classifier(data[100], orange.GetBoth)</code>
(<orange.Value 'iris'='Iris-virginica'>, <0.000, 0.001, 0.999>)
>>>
</pre>

<p>For a more elaborate test that also shows the use of a learner
(that is not given the data at its initialization), here is a
script that does 10-fold cross validation:</p>

<p class="header">
<a href=
"nbdisc_test.py">nbdisc_test.py</a>
(uses <a href="iris.tab">iris.tab</a> and
<a href="nbdisc.py">nbdisc.py</a>)</p>
<xmp class="code">import orange, orngEval, nbdisc
data = orange.ExampleTable("iris")
results = orngEval.CrossValidation([nbdisc.Learner()], data)
print "Accuracy = %5.3f" % orngEval.CA(results)[0]
</xmp>

<p>The accuracy on this data set is about 92%. You may try to obtain a
better accuracy by using some other type of discretization, or try
some other learner on this data (hint: k-NN should perform
better).</p>



<h2>Python Implementation of Naive Bayesian Classifier</h2>
<index name="naive Bayesian classifier (in Python)">

<p>The naive Bayesian classifier we will implement in
this lesson uses standard naive Bayesian algorithm also described
in Michell: Machine Learning, 1997 (pages 177-180). Essentially, if
an instance is described with n attributes a<sub>i</sub> (i from 1
to n), then the class that instance is classified to a class v from
set of possible classes V according to naive Bayes classifier
is:</p>

<center><img src="f1.gif" alt="formula for v"></center>

<p>We will also compute a vector of elements</p>

<center><img src="f2.gif" alt="formula for pj"></center>

<p>which, after normalization so that the sum of p<sub>j</sub> is
equal to 1, represent class probabilities. The class probabilities and
conditional probabilities (priors) in above formulas are estimated
from training data: class probability is equal to the relative class
frequency, while the conditional probability of attribute value given
class is computed by figuring out the proportion of instances with a
value of i-th attribute equal to a<sub>i</sub> among instances that
from class v<sub>j</sub>.</p>

<p>To complicate things just a little bit, m-estimate (see
Mitchell, and Cestnik IJCAI-1990) will be used instead of relative
frequency when computing prior conditional probabilities. So
(following the example in Mitchell), when assessing
P=P(Wind=strong|PlayTennis=no) we find that the total number of
training examples with PlayTennis=no is n=5, and of these there are
nc=3 for which Wind=strong, than using relative frequency the
corresponding probability would be</p>

<center><img src="f3.gif" alt="formula for P"></center>

<p>Relative frequency has a problem when number of instance is
small, and to alleviate that m-estimate assumes that there are m
imaginary cases (m is also referred to as equivalent sample size)
with equal probability of class values p. Our conditional
probability using m-estimate is then computed as</p>

<center><img src="f4.gif" alt="formula for Pm"></center>

<p>Often, instead of uniform class probability p, a relative class
frequency as estimated from training data is taken.</p>

<p></p>

<p>We will develop a module called bayes.py that will implement our
naive Bayes learner and classifier. The structure of the module
will be as with <a href="c_nb_disc.htm">previous example</a>.
Again, we will implement two classes, one for learning and the other
on for classification. Here is a <code>Learner</code>: class</p>

<p class="header">class Learner_Class from <a href=
"bayes.py">bayes.py</a></p>
<xmp class="code">class Learner_Class:
  def __init__(self, m=0.0, name='std naive bayes', **kwds):
    self.__dict__.update(kwds)
    self.m = m
    self.name = name

  def __call__(self, examples, weight=None, **kwds):
    for k in kwds.keys():
      self.__dict__[k] = kwds[k]
    domain = examples.domain

    # first, compute class probabilities
    n_class = [0.] * len(domain.classVar.values)
    for e in examples:
      n_class[int(e.getclass())] += 1

    p_class = [0.] * len(domain.classVar.values)
    for i in range(len(domain.classVar.values)):
      p_class[i] = n_class[i] / len(examples)

    # count examples with specific attribute and
    # class value, pc[attribute][value][class]

    # initialization of pc
    pc = []
    for i in domain.attributes:
      p = [[0.]*len(domain.classVar.values) for i in range(len(i.values))]
      pc.append(p)

    # count instances, store them in pc
    for e in examples:
      c = int(e.getclass())
      for i in range(len(domain.attributes)):
      if not e[i].isSpecial():
        pc[i][int(e[i])][c] += 1.0

    # compute conditional probabilities
    for i in range(len(domain.attributes)):
      for j in range(len(domain.attributes[i].values)):
        for k in range(len(domain.classVar.values)):
          pc[i][j][k] = (pc[i][j][k] + self.m * p_class[k])/ \
            (n_class[k] + self.m)

    return Classifier(m = self.m, domain=domain, p_class=p_class, \
             p_cond=pc, name=self.name)
</xmp>

<p>Initialization of Learner_Class saves the two attributes, m and
name of the classifier. Notice that both parameters are optional,
and the default value for m is 0, making naive Bayes m-estimate
equal to relative frequency unless the user specifies some other
value for m. Function <code>__call__</code> is called with the training data
set, computes class and conditional probabilities and calls
classifiers, passing the probabilities along with some other
variables required for classification.</p>

<p class="header">class Classifier from <a href="bayes.py">bayes.py</a></p>
<xmp class="code">class Classifier:
  def __init__(self, **kwds):
    self.__dict__.update(kwds)

  def __call__(self, example, result_type=orange.GetValue):
    # compute the class probabilities
    p = map(None, self.p_class)
    for c in range(len(self.domain.classVar.values)):
      for a in range(len(self.domain.attributes)):
        if not example[a].isSpecial():
          p[c] *= self.p_cond[a][int(example[a])][c]

    # normalize probabilities to sum to 1
    sum =0.
    for pp in p: sum += pp
    if sum>0:
      for i in range(len(p)): p[i] = p[i]/sum

    # find the class with highest probability
    v_index = p.index(max(p))
    v = orange.Value(self.domain.classVar, v_index)

    # return the value based on requested return type
    if result_type == orange.GetValue:
      return v
    if result_type == orange.GetProbabilities:
      return p
    return (v,p)

  def show(self):
    print 'm=', self.m
    print 'class prob=', self.p_class
    print 'cond prob=', self.p_cond
</xmp>


<p>Upon first invocation, the classifier will store the values of
the parameters it was called with (<code>__init__</code>). When called with a
data instance, it will first compute the class probabilities using
the prior probabilities sent by the learner. The probabilities will
be normalized to sum to 1. The class will then be found that has
the highest probability, and the classifier will accordingly
predict to this class. Notice that we have also added a method
called show, which reports on m, class probabilities and
conditional probabilities:</p>

<p class="header">uses <a href="voting.tab">voting.tab</a></p>
<pre class="code">
> <strong>python</strong>
>>> <strong>import orange, bayes</strong>
>>> <strong>data = orange.ExampleTable("voting")</strong>
>>> <strong>classifier = bayes.Learner(data)</strong>
>>> <strong>classifier.show()</strong>
m= 0.0
class prob= [0.38620689655172413, 0.61379310344827587]
cond prob= [[[0.79761904761904767, 0.38202247191011235], ...]]
>>>
</pre>


<p>The following script tests our naive Bayes, and compares it to
10-nearest neighbors. Running the script (do you it yourself)
reports classification accuracies just about 90% (somehow, on this
data set, kNN does better; smrc&hellip;).</p>

<p class="header"><a href="bayes_test.py">bayes_test.py</a> (uses <a href="bayes.py">bayes.py</a> and <a href="voting.tab">voting.tab</a>)</p>
<xmp class="code">import orange, orngEval, bayes
data = orange.ExampleTable("voting")

bayes = bayes.Learner(m=2, name='my bayes')
knn = orange.kNNLearner(k=10)
knn.name = "knn"

learners = [knn,bayes]
results = orngEval.CrossValidation(learners, data)
for i in range(len(learners)):
    print learners[i].name, orngEval.CA(results)[i]
</xmp>



<h2>Bagging</h2>

<p>Here we show how to use the schema that allows us to build our own
learners/classifiers for bagging. While you can find bagging,
boosting, and other ensemble-related stuff in <a
href="../modules/orngEnsemble.htm">orngEnsemble</a> module, we thought
explaining how to code bagging in Python may provide for a nice
example. The following pseudo-code (from
Whitten &amp; Frank: Data Mining) illustrates the main idea of bagging:</p>

<xmp class="code">MODEL GENERATION
Let n be the number of instances in the training data.
For each of t iterations:
   Sample n instances with replacement from training data.
   Apply the learning algorithm to the sample.
   Store the resulting model.

CLASSIFICATION
For each of the t models:
   Predict class of instance using model.
Return class that has been predicted most often.
</xmp>


<p>Using the above idea, this means that our <code>Learner_Class</code> will
need to develop t classifiers and will have to pass them to
<code>Classifier</code>, which, once seeing a data instance, will use them for
classification. We will allow parameter t to be specified by the
user, 10 being the default.</p>

<p>The code for the <code>Learner_Class</code> is therefore:</p>

<p class="header">class <code>Learner_Class</code> from <a href=
"bagging.py">bagging.py</a></p>
<xmp class="code">class Learner_Class:
    def __init__(self, learner, t=10, name='bagged classifier'):
        self.t = t
        self.name = name
        self.learner = learner

    def __call__(self, examples, weight=None):
        n = len(examples)
        classifiers = []
        for i in range(self.t):
            selection = []
            for i in range(n):
                selection.append(random.randrange(n))
            data = examples.getitems(selection)
            classifiers.append(self.learner(data))
            
        return Classifier(classifiers = classifiers, \
            name=self.name, domain=examples.domain)
</xmp>

<p>Upon invocation, <code>__init__</code> stores the base learning
(the one that will be bagged), the value of the parameter t, and the
name of the classifier. Note that while the learner requires the base
learner to be specified, parameters t and name are optional.</p>

<p>When the learner is called with examples, a list of t classifiers
is build and stored in variable <code>classifier</code>. Notice that
for data sampling with replacement, a list of data instance indices is
build (<code>selection</code>) and then used to sample the data from
training examples (<code>example.getitems</code>). Finally, a
<code>Classifier</code> is called with a list of classifiers, name and
domain information.</p>

<p class="header">class <code>Classifier</code> from <a href=
"bagging.py">bagging.py</a></p>
<xmp class="code">class Classifier:
    def __init__(self, **kwds):
        self.__dict__.update(kwds)

    def __call__(self, example, resultType = orange.GetValue):
        freq = [0.] * len(self.domain.classVar.values)
        for c in self.classifiers:
            freq[int(c(example))] += 1
        index = freq.index(max(freq))
        value = orange.Value(self.domain.classVar, index)
        for i in range(len(freq)):
            freq[i] = freq[i]/len(self.classifiers)
        if resultType == orange.GetValue: return value
        elif resultType == orange.GetProbabilities: return freq
        else: return (value, freq)
</xmp>


<p>For initialization, <code>Classifier</code> stores all parameters
it was invoked with. When called with a data instance, a list freq is
initialized which is of length equal to the number of classes and
records the number of models that classify an instance to a specific
class. The class that majority of models voted for is returned. While
it may be possible to return classes index, or even a name, by
convention classifiers in Orange return an object <code>Value</code>
instead.</p>

<p>Notice that while, originally, bagging was not intended to compute
probabilities of classes, we compute these as the proportion of models
that voted for a certain class (this is probably incorrect, but
suffice for our example, and does not hurt if only classes values and
not probabilities are used).</p>

<p>Here is the code that tests our bagging we have just
implemented. It compares a decision tree and its bagged variant.  Run
it yourself to see which one is better!</p>

<p class="header"><a href="bagging_test.py">bagging_test.py</a> (uses <a
href="bagging.py">bagging.py</a> and <a href=
"../datasets/adult_sample.tab">adult_sample.tab</a>)</p>
<xmp class="code">import orange, orngTree, orngEval, bagging
data = orange.ExampleTable("adult_sample")

tree = orngTree.TreeLearner(mForPrunning=10, minExamples=30)
tree.name = "tree"
baggedTree = bagging.Learner(learner=tree, t=5)

learners = [tree, baggedTree]

results = orngEval.crossValidation(learners, data, folds=5)
for i in range(len(learners)):
    print learners[i].name, orngEval.CA(results)[i]
</xmp>




<hr><br><p class="Path">
Prev: <a href="c_performance.htm">Testing and Evaluating</a>,
Next: <a href="regression.htm">Regression</a>,
Up: <a href="classification.htm">Classification</a>
</p>

</body>
</html>


<html><HEAD>
<LINK REL=StyleSheet HREF="../style.css" TYPE="text/css">
</HEAD>
<body>

<p class="Path">
Prev: <a href="c_pythonlearner.htm">Build Your Own Learner</a>,
Next: <a href="c_nb.htm">Naive Bayes in Python</a>,
Up: <a href="c_pythonlearner.htm">Build Your Own Learner</a>,
</p>

<H1>Naive Bayes with Discretization</H1>

<p>Let us build a learner/classifier that is an extension of build-in
naive Bayes and which before learning categorizes the data (see also
the lesson on <a href= "o_categorization.htm">Categorization</a>). We
will define a module <a href="nbdisc.py">nbdisc.py</a> that will
implement our method. As we have explained in the <a href=
"c_pythonlearner.htm">introductory text on learners/classifiers</a>,
it will implement two classes, Learner and Classifier. First,
here is definition of a Learner class:</p>

<p class="header">function <code>Learner</code> from <a href=
"nbdisc.py">nbdisc.py</a></p>
<xmp class="code">class Learner(object):
    def __new__(cls, examples=None, name='discretized bayes', **kwds):
        learner = object.__new__(cls, **kwds)
        if examples:
            learner.__init__(name) # force init
            return learner(examples)
        else:
            return learner  # invokes the __init__

    def __init__(self, name='discretized bayes'):
        self.name = name

    def __call__(self, data, weight=None):
        disc = orange.Preprocessor_discretize( \
            data, method=orange.EntropyDiscretization())
        model = orange.BayesLearner(disc, weight)
        return Classifier(classifier = model)
</xmp>

<p>Learner_Class has three methods. Method <code>__new__</code>
creates the object and returns a learner or classifier, depending if
examples where passed to the call. If the examples were passed as an
argument than the method called the learner (invoking
<code>__call__</code> method). Method <code>__init__</code> is invoked
every time the class is called for the first time. Notice that all it
does is remembers the only argument that this class can be called
with, i.e. the argument <code>name</code> which defaults to
&lsquo;discretized bayes&rsquo;. If you would expect any other
arguments for your learners, you should handle them here (store them
as class&rsquo; attributes using the keyword <code>self</code>).</p>

<p>If we have created an instance of the learner (and did not pass the
examples as attributes), the next call of this learner will invoke a
method <code>__call__</code>, where the essence of our learner is
implemented. Notice also that we have included an attribute for vector
of instance weights, which is passed to naive Bayesian learner. In our
learner, we first discretize the data using Fayyad &amp; Irani&rsquo;s
entropy-based discretization, then build a naive Bayesian model and
finally pass it to a class <code>Classifier</code>. You may expect
that at its first invocation the <code>Classifier</code> will just
remember the model we have called it with:</p>

<p class="header">class Classifier from <a href=
"nbdisc.py">nbdisc.py</a></p>
<xmp class="code">class Classifier:
    def __init__(self, **kwds):
        self.__dict__.update(kwds)

    def __call__(self, example, resultType = orange.GetValue):
        return self.classifier(example, resultType)
</xmp>


<p>The method <code>__init__</code> in <code>Classifier</code> is
rather general: it makes <code>Classifier</code> remember all
arguments it was called with. They are then accessed through
<code>Classifiers</code>&rsquo; arguments
(<code>self.argument_name</code>). When Classifier is called, it
expects an example and an optional argument that specifies the type of
result to be returned.</p>

<p>This comples our code for naive Bayesian classifier with
discretization. You can see that the code is fairly short (fewer than 
20 lines), and it can be easily extended or changed if we want to do
something else as well (include a feature subset selection, for instance,
&hellip;).</p>

<p>Here are now a few lines to test our code:</p>

<p class="header">uses <a href="iris.tab">iris.tab</a> and <a href=
"nbdisc.py">nbdisc.py</a></p>
<pre class="code">
> <code>python</code>
>>> <code>import orange, nbdisc</code>
>>> <code>data = orange.ExampleTable("iris")</code>
>>> <code>classifier = nbdisc.Learner(data)</code>
>>> <code>print classifier(data[100])</code>
Iris-virginica
>>> <code>classifier(data[100], orange.GetBoth)</code>
(<orange.Value 'iris'='Iris-virginica'>, <0.000, 0.001, 0.999>)
>>>
</pre>

<p>For a more elaborate test that also shows the use of a learner
(that is not given the data at its initialization), here is a
script that does 10-fold cross validation:</p>

<p class="header">
<a href=
"nbdisc_test.py">nbdisc_test.py</a>
(uses <a href="iris.tab">iris.tab</a> and
<a href="nbdisc.py">nbdisc.py</a>)</p>
<xmp class="code">import orange, orngEval, nbdisc
data = orange.ExampleTable("iris")
results = orngEval.CrossValidation([nbdisc.Learner()], data)
print "Accuracy = %5.3f" % orngEval.CA(results)[0]
</xmp>

<p>The accuracy on this data set is about 92%. You may try to obtain a
better accuracy by using some other type of discretization, or try
some other learner on this data (hint: k-NN should perform
better).</p>

<p>You can now read on to see how the same schema of developing
your own classifier was used for to assemble all-in-python <a href=
"c_nb.htm">naive Bayesian method</a> and see how it is very easy to
implement <a href="c_bagging.htm">bagging</a>.</p>

<hr><br><p class="Path">
Prev: <a href="c_pythonlearner.htm">Build Your Own Learner</a>,
Next: <a href="c_nb.htm">Naive Bayes in Python</a>,
Up: <a href="c_pythonlearner.htm">Build Your Own Learner</a>,
</p>

</body>
</html>


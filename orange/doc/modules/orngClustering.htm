<html><HEAD>
<LINK REL=StyleSheet HREF="../style.css" TYPE="text/css">
</HEAD>
<body>
<h1>orngClustering: Partitional and Agglomerative Clustering</h1>

<index name="modules+clustering">

<p>The module implements a k-means partitional clustering, and provides a wrapper around Orange's implementation of agglomerative hierarchical clustering. The module also implements a number of useful functions associated with these two clustering methods, like leaf-ordering of the dendrogram and dendrogram plot.</p>

<h2>KMeans</h2>

<p>Class <code>KMeans</code> provides for an implementation of standard k-means clustering algorithm:</p>
<ol>
<li>Choose the number of clusters, k.</li>
<li>Choose a set of k initial centroids.</li>
<li>Assign each instances in the data set to the closest centroid.</li>
<li>For each cluster, compute a new centroid as a center of clustered data instances.</li>
<li>Repeat the previous two steps, until some convergence criterion is met (e.g., the cluster assignment has not changed).</li>
</ol>

<p>The main advantage of the algorithm is simplicity and low memory space requirements. The principal disadvantage is the dependence of results on the selection of initial set of centroids.</p>

<P class=section>Methods</P>
<DL class=attributes>
<DT>__init__(data=None, centroids=3, maxiters=None, minscorechange=None, stopchanges=0, nstart=1, initialization=kmeans_init_random, distance=orange.ExamplesDistanceConstructor_Euclidean,
scoring=score_distance_to_centroids,
inner_callback = None,
outer_callback = None,
initialize_only = False)</DT>
<DD><code>data</code> is an Orange's ExampleTable object that stores the data instances to be clustered. If <code>data</code> is not <code>None</code>, clustering will immediately executed after the initialization of clustering parameters unless <code>initialize_only</code> is set to <code>True</code>. <code>centroids</code> either specify a number of clusters or provide a list of examples that will serve as clustering centroids. The clustering will stop if one of the following conditions is met: the number of clustering iterations exceeds <code>maxiters</code>, the number of instances changing the cluster is equal to <code>stopchanges</code>, or the score associated with current clustering improved for less than <code>minscorechange</code> of the score from previous iteration. If <code>minscorechange</code> is not set, the score will not be computed between iterations. User can also provide a example distance constructor, which, given the data set, will provide a function that measures the distance between two example instances (see <a href="../reference/ExamplesDistance.htm">Distances between Examples</a>). A function to select centroids given the table of data instances, k and a example distance function is provided by <code>initialization</code>, the module includes implementations of several different approaches. <code>scoring</code> is a function that takes clustering object as an argument and returns a score for the clustering. It could be used, for instance, in procedure that repeats the clustering <code>nstart</code> times, returning the clustering with the lowest score. The two callbacks are invoked either after every clustering iteration (<code>inner_callback</code>) or after every clustering restart (in case when <code>nstart</code> is greater than 1, <code>outer_callback</code>).<dd>
<DT>runone()</DT>
<DD>Runs one clustering iteration, starting with re-computation of centroids, followed by computation of data membership (associating data instances to their nearest centroid).</DD>
<DT>run()</DT>
<DD>Runs clustering until the convergence conditions are met. If <code>nstart</code> is greater than one, <code>nstart</code> runs of the clustering algorithm will be executed, returning the clustering with the best (lowest) score.</DD>
</DL>

<p class=section>Attributes</P>
<DL class=attributes>
<DT>k</DT>
<DD>Number of clusters.</DD>
<DT>centroids</DT>
<DD>Current set of centroids.
<DT>scoring</DT>
<DD>Current clustering score.</DD>
<DT>iteration</DT>
<DD>Current clustering iteration.</DD>
<DT>clusters</DT>
<DD>A list of cluster indexes. An i-th element provides an index to a centroid associated with i-th data instance from the input data set.</DD>
</DL>

<p>The following code runs k-means clustering on Iris data set and prints out the cluster indexes for the last 10 data instances:</p>

<p class="header">part of <a href="kmeans-run.py">kmeans-run.py</a> (uses <a href=
"iris.tab">iris.tab</a>)</p>
<xmp class=code>data = orange.ExampleTable("iris")
km = orngClustering.KMeans(data, 3)
print km.clusters[-10:]
</xmp>

<p>The output of this code is:</p>
<xmp class=code>[1, 1, 2, 1, 1, 1, 2, 1, 1, 2]
</xmp>

<p>Invoking a call-back function may be useful when tracing the progress of the clustering. Below is a code that uses an <code>inner_callback</code> to report on the number of instances that have changed the cluster and to report on the clustering score. For the score to be computed at each iteration we have to set <code>minscorechange</code>, but we can leave it at 0 (or even set it to a negative value, which allows the score to deteriorate by some amount).</p>

<p class="header">part of <a href="kmeans-run-callback.py">kmeans-run-callback.py</a> (uses <a href=
"iris.tab">iris.tab</a>)</p>
<xmp class=code>def callback(km):
    print "Iteration: %d, changes: %d, score: %.4f" % (km.iteration,
        km.nchanges, km.score)
    
data = orange.ExampleTable("iris")
km = orngClustering.KMeans(data, 3, minscorechange=0, inner_callback=callback)
</xmp>

<p>The convergence on Iris data set is fast:</p>
<xmp class=code>Iteration: 1, changes: 150, score: 10.9555
Iteration: 2, changes: 12, score: 10.3867
Iteration: 3, changes: 2, score: 10.2034
Iteration: 4, changes: 2, score: 10.0699
Iteration: 5, changes: 2, score: 9.9542
Iteration: 6, changes: 1, score: 9.9168
Iteration: 7, changes: 2, score: 9.8624
Iteration: 8, changes: 0, score: 9.8624
</xmp>


<p>Call-back above is used for reporting of the progress, but may as well call a function that plots a selection data projection with corresponding centroid at a given step of the clustering. This is exactly what we did with the following script:</p>

<p class="header">part of <a href="kmeans-trace.py">kmeans-trace.py</a> (uses <a href=
"iris.tab">iris.tab</a>)</p>
<xmp class=code>def plot_scatter(data, km, attx, atty, filename="kmeans-scatter", title=None):
    """plot a data scatter plot with the position of centeroids"""
    pylab.rcParams.update({'font.size': 8, 'figure.figsize': [4,3]})
    x = [float(d[attx]) for d in data]
    y = [float(d[atty]) for d in data]
    colors = ["c", "w", "b"]
    cs = "".join([colors[c] for c in km.clusters])
    pylab.scatter(x, y, c=cs, s=10)
    
    xc = [float(d[attx]) for d in km.centroids]
    yc = [float(d[atty]) for d in km.centroids]
    pylab.scatter(xc, yc, marker="x", c="k", s=200)
    
    pylab.xlabel(attx)
    pylab.ylabel(atty)
    if title:
        pylab.title(title)
    pylab.savefig("%s-%03d.png" % (filename, km.iteration))
    pylab.close()

def in_callback(km):
    print "Iteration: %d, changes: %d, score: %8.6f" % (km.iteration, km.nchanges, km.score)
    plot_scatter(data, km, "petal width", "petal length", title="Iteration %d" % km.iteration)
    
data = orange.ExampleTable("iris")
random.seed(42)
km = orngClustering.KMeans(data, 3, maxiters=10, inner_callback=in_callback)</xmp>

<p>Only the first four scatterplots are shown below. Colors of the data instances indicate the cluster membership. Notice that since the Iris data set includes four attributes, the closest centroid in a particular 2-dimensional projection is not necessary also the centroid of the cluster that the data point belongs to.</p>

<table>
<tr>
<td><img src="kmeans-scatter-001.png"></td>
<td><img src="kmeans-scatter-002.png"></td>
</tr>
<tr>
<td><img src="kmeans-scatter-003.png"></td>
<td><img src="kmeans-scatter-004.png"></td>
</tr>
</table>


<h2>k-Means Utility Functions</h2>

<dl class="attributes">
<dt>kmeans_init_random(data, k, _)</dt>
<dd class="ddfun">A function that can be used for initialization of k-means clustering returns <code>k</code> data instances from the <code>data</code>. This type of initialization is also known as Fory's initialization (Forgy, 1965; He et al., 2004).</dd>
<dt>kmeans_init_diversity(data, k, distfun)</dt>
<dd class="ddfun">Another function that can be used for intializationof k-means clustering. Given the data set, number of clusters and adistance function returns a set of centroids where the first one is a data point being the farthest away from the center of the data, and consequent centroids data points of which the minimal distance to the previous set of centroids is maximal. This type of initialization is almost identical to initialization proposed by Katsavounidis et al. (1994), with the only difference being in the selection of the first centroid (where they use a data instance with the biggest norm).</dd>
<dt>KMeans_init_hierarchicalClustering(n=100)</dt>
<dd class="ddfun">Is actually a class that returns an clustering initialization function that, given the data, k, and a distance function samples <code>n</code> data instances, performs hierarhical clustering, uses it to infer <code>k</code> clusters, and computes a list of cluster-based data centers.</dd>
<dt>data_center(data)</dt>
<dd class="ddfun">Returns a center of the instances in the data set (average across data instances for continuous attributes, most frequent value for discrete attributes).</dd>
<dt>score_distance_to_centroids(kmeans)</dt>
<dd class="ddfun">Returns an average distance of data instances to their associated centroids. <code>kmeans</code> is a k-means clustering object.</dd>
<dt>score_silhouette(kmeans, index=None)</dt>
<dd class="ddfun">Returns an average silhouette score of data instances. If <code>index</code> is specified it instead returns just the silhouette score of that particular data instance. <code>kmeans</code> is a k-means clustering object.</dd>
<dt>plot_silhouette(kmeans, filename='tmp.png')</dt>
<dd class="ddfun">Saves a silhuette plot to <code>filename</code>, showing the distributions of silhouette scores in clusters. <code>kmeans</code> is a k-means clustering object.</dd>
</dl>

<p>Typically, the choice of seeds has a large impact on the k-means clustering, with better initialization methods yielding a clustering that converges faster and finds more optimal centroids. The following code compares three different initialization methods (random, diversity-based and hierarchical clustering-based) in terms of how fast they converge:</p>

<p class="header">part of <a href="kmeans-cmp-init.py">kmeans-cmp-init.py</a> (uses <a href=
"iris.tab">iris.tab</a>, <a href=
"housing.tab">housing.tab</a>, <a href=
"vehicle.tab">vehicle.tab</a>)</p>
<xmp class=code>import orange
import orngClustering
import random

data_names = ["iris", "housing", "vehicle"]
data_sets = [orange.ExampleTable(name) for name in data_names]

print "%10s %3s %3s %3s" % ("", "Rnd", "Div", "HC")
for data, name in zip(data_sets, data_names):
    random.seed(42)
    km_random = orngClustering.KMeans(data, centroids = 3)
    km_diversity = orngClustering.KMeans(data, centroids = 3, \
        initialization=orngClustering.kmeans_init_diversity)
    km_hc = orngClustering.KMeans(data, centroids = 3, \
        initialization=orngClustering.KMeans_init_hierarchicalClustering(n=100))
    print "%10s %3d %3d %3d" % (name, km_random.iteration, km_diversity.iteration, km_hc.iteration)
</xmp>

<p>The results show that diversity and clustering-based initialization make k-means converge faster that random selection of seeds (as expected):</p>

<xmp class=code>           Rnd Div  HC
      iris  12   3   4
   housing  14   6   4
   vehicle  11   4   3
</xmp>

<h2>Hierarchical Clustering</h2>

<dl class="attributes">
<dt>hierarchicalClustering(data,                     distanceConstructor=orange.ExamplesDistanceConstructor_Euclidean, linkage=orange.HierarchicalClustering.Average, order=False, progressCallback=None)</dt>
<dd class="ddfun">Returns an object with information of a hierarchical clustering of a given data set. This is a wrapper function around <a href="../reference/clustering.htm">HierarchicalClustering</a> class implemented in Orange. <code>hierarchicalClustering</code> uses <code>distanceConstructor</code> function (see <a href="../reference/ExamplesDistance.htm">Distances between example</a>) to construct a distance matrix, which is then passed to Orange's hierarchical clustering algorithm, along with a particular linkage method. Ordering of leaves can be requested (<code>order=True</code>) and if so, the leaves will be ordered using <code>orderLeaves</code> function (see below).</dd>

<dt>orderLeaves(root, distanceMatrix)</dt>
<dd class="ddfun">Given the object with hierarchical clustering (a root node of the tree) and a distance matrix, function uses a fast optimal leaf ordering by Bar-Joseph et al. to impose the order of the branches in the dendrogram so that the distance between the neighboring leaves is minimized.</dd>

<dt>orderLeaves(root, distanceMatrix)</dt>
<dd class="ddfun">Given the object with hierarchical clustering (a root node of the tree) and a distance matrix, function uses a fast optimal leaf ordering by Bar-Joseph et al. to impose the order of the branches in the dendrogram so that the distance between the neighboring leaves is minimized.</dd>

<dt>hierarchicalClustering_topClusters(root, k)</dt>
<dd class="ddfun">Returns k topmost clusters (top k nodes of the clustering tree) from hierarchical clustering.</dd>

<dt>hierarhicalClustering_topClustersMembership(root, k)</dt>
<dd class="ddfun">Returns a list with indexes which indicate the membership of data instances used to create the clustering to top k clusters.</dd>
</dl>


<p>Using <code>hierarchicalClustering</code>, scripts need a single line of code to invoke the clustering and get the object with a result. This is demonstrated in the following script, that considers the Iris data set, performs hierarchical clustering, and then plots the data in two-attribute projection, coloring the points representing data instances according to cluster membership.</p>

<p class="header">part of <a href="hclust-iris.py">hclust-iris.py</a> (uses <a href=
"iris.tab">iris.tab)</a></p>
<xmp class=code>def plot_scatter(data, cls, attx, atty, filename="hclust-scatter", title=None):
    """plot a data scatter plot with the position of centeroids"""
    pylab.rcParams.update({'font.size': 8, 'figure.figsize': [4,3]})
    x = [float(d[attx]) for d in data]
    y = [float(d[atty]) for d in data]
    colors = ["c", "w", "b"]
    cs = "".join([colors[c] for c in cls])
    pylab.scatter(x, y, c=cs, s=10)
    
    pylab.xlabel(attx)
    pylab.ylabel(atty)
    if title:
        pylab.title(title)
    pylab.savefig("%s.png" % filename)
    pylab.close()

data = orange.ExampleTable("iris")
root = orngClustering.hierarchicalClustering(data)
n = 3
cls = orngClustering.hierarhicalClustering_topClustersMembership(root, n)
plot_scatter(data, cls, "sepal width", "sepal length", title="Hiearchical clustering (%d clusters)" % n)
</xmp>

<p>The output of the script is a following plot:</p>

<img src="hclust-scatter.png">


<h2>DendrogramPlot</h2>
<p>Class <code>DendrogramPlot</code> implements visualization of the  clustering tree (called dendrogram) and corresponding visualization of attribute heatmap.

<p class=section>Methods</p>
<dl class=attributes>
<dt>__init__(tree, data=None, labels=None, width=None, height=None, treeAreaWidth=None, textAreaWidth=None, matrixAreaWidth=None, fontSize=None, lineWidth=2, clusterColors={})</DT>
<dd><code>tree</code> is an Orange's hierarhical clustering tree object (root node). If <code>data</code> (Orange's ExampleTable) is given than the dendrogram will include a heat map with color-based presentation of attribute's values. The length of the data set should match the number of leaves in the hierarchical clustering tree. Following are arguments that define the height and width of the plot areas, font size for the labels, and width of lines that indicate branches of the tree. Branches are plotted in black, but may be color to visually expose various clusters. The coloring is specified with <code>clusterColors</code>, a dictionary with cluster instances as keys and (r, g, b) tuples as items.</dd>

<dt>plot(filename="graph.png")</dt>
<dd>Plots the dendrogram and save is to the output file.</dd>
</dl>


<p>To illustrate the use of the dendrogram plotting class, the following scripts uses it on a subset of 20 instances from the Iris data set. Values of the class variables is used for labeling the leaves (and, of course, it is not used for the clustering - only the non-class attributes are used to compute instance distance matrix).</p>

<p class="header">part of <a href="hclust-dendrogram.py">hclust-dendrogram.py</a> (uses <a href=
"iris.tab">iris.tab)</a></p>
<xmp class=code>data = orange.ExampleTable("iris")
sample = data.selectref(orange.MakeRandomIndices2(data, 20), 0)
root = orngClustering.hierarchicalClustering(sample)
dendrogram = orngClustering.DendrogramPlot(root, sample, labels=[str(d.getclass()) for d in sample])
dendrogram.plot("hclust-dendrogram.png")
</xmp>

<p>The resulting dendrogram is shown below.</p>

<img src="hclust-dendrogram.png">

<p>Following is a similar script to above one, but this time we have 1) distinctively colored the three topmost dendrogram branches, 2) used a color schema for representation of attribute values (default, as above, are shades of gray, in the following script the colors would span between green and red), and 3) included only two attributes in the heat map presentation (note: clustering is still done on all of the data set's attributes).</p>

<p class="header">part of <a href="hclust-colored-dendrogram.py">hclust-colored-dendrogram.py</a> (uses <a href=
"iris.tab">iris.tab</a></p>
<xmp class=code>data = orange.ExampleTable("iris")
sample = data.selectref(orange.MakeRandomIndices2(data, 20), 0)
root = orngClustering.hierarchicalClustering(sample)
reduced = orange.ExampleTable(orange.Domain(sample.domain[:2], False), sample)

my_colors = [(255,0,0), (0,255,0), (0,0,255)]
cls = orngClustering.hierarchicalClustering_topClusters(root, 3)
colors = dict([(cl, col) for cl, col in zip(cls, my_colors)])

dendrogram = orngClustering.DendrogramPlot(root, reduced, labels=[str(d.getclass()) for d in sample],
    clusterColors=colors)
dendrogram.setMatrixColorScheme((0, 255, 0), (255, 0, 0))
dendrogram.plot("hclust-colored-dendrogram.png")
</xmp>

<p>Our "colored" dendrogram is now saved as shown in the figure below:</p>

<img src="hclust-colored-dendrogram.png">

<h2>References</h2>

<p>Forgy E (1965) Cluster analysis of multivariate data: Efficiency versus interpretability of classification. Biometrics 21(3): 768-769.</p>

<p>He J, Lan M, Tan C-L , Sung S-Y, Low H-B (2004) <a href="http://www.comp.nus.edu.sg/~tancl/Papers/IJCNN04/he04ijcnn.pdf">Initialization of cluster refinement algorithms: A review and comparative study</a>. In Proceedings of International Joint Conference on Neural Networks (IJCNN), pages 297-302, Budapest, Hungary.</p>

<p>Katsavounidis I, Jay C, Zhang Z (1994) A new initialization technique for generalized Lloyd iteration. IEEE Signal Processing Letters 1(10): 144-146.</p>

<p>Bar-Joseph Z, Gifford DK, Jaakkola TS (2001) <a href="http://bioinformatics.oxfordjournals.org/cgi/content/abstract/17/suppl_1/S22">Fast optimal leaf ordering for herarchical clustering</a>. Bioinformatics 17(Suppl. 1): S22-S29.
</body>
</html>

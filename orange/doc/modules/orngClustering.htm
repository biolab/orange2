<html><HEAD>
<LINK REL=StyleSheet HREF="../style.css" TYPE="text/css">
</HEAD>
<body>
<h1>orngClustering: Partitional and Agglomerative Clustering</h1>

<index name="modules+clustering">

<p>The module implements a k-means partitional clustering, and provides a wrapper around Orange's implementation of agglomerative hierarchical clustering. The module also implements a number of useful functions associated with these two clustering methods, like leaf-ordering of the dendrogram and dendrogram plot.</p>

<h2>KMeans</h2>

<p>Class <code>KMeans</code> provides for an implementation of standard k-means clustering algorithm:</p>
<ol>
<li>Choose the number of clusters, k.</li>
<li>Choose a set of k initial centroids.</li>
<li>Assign each instances in the data set to the closest centroid.</li>
<li>For each cluster, compute a new centroid as a center of clustered data instances.</li>
<li>Repeat the previous two steps, until some convergence criterion is met (e.g., the cluster assignment has not changed).</li>
</ol>

<p>The main advantage of the algorithm is simplicity and low memory space requirements. The principal disadvantage is the dependence of results on the selection of initial set of centroids.</p>

<P class=section>Methods</P>
<DL class=attributes>
<DT>__init__(data=None, centroids=3, maxiters=None, maxscorechange=None, stopchanges=0, nstart=1, initialization=kmeans_init_random, distance=orange.ExamplesDistanceConstructor_Euclidean,
scoring=score_distanceToCentroids,
inner_callback = None,
outer_callback = None,
initialize_only = False)</DT>
<DD><code>data</code> is an Orange's ExampleTable object that stores the data instances to be clustered. If <code>data</code> is not <code>None</code>, clustering will immediately executed after the initialization of clustering parameters unless <code>initialize_only</code> is set to <code>True</code>. <code>centroids</code> either specify a number of clusters or provide a list of examples that will serve as clustering centroids. The clustering will stop if one of the following conditions is met: the number of clustering iterations exceeds <code>maxiters</code>, the number of instances changing the cluster is equal to <code>stopchanges</code>, or the score associated with current clustering changed for less than <code>maxscorechange</code> from the clustering of the previous iteration. User can also provide a example distance constructor, which, given the data set, will provide a function that measures the distance between two example instances (see <a href="../reference/ExamplesDistance.htm">Distances between Examples</a>). A function to select centroids given the table of data instances, k and a example distance function is provided by <code>initialization</code>, the module includes implementations of several different approaches. <code>scoring</code> is a function that takes clustering object as an argument and returns a score for the clustering. It could be used, for instance, in procedure that repeats the clustering <code>nstart</code> times, returning the clustering with the lowest score. The two callbacks are invoked either after every clustering iteration (<code>inner_callback</code>) or after every clustering restart (in case when <code>nstart</code> is greater than 1, <code>outer_callback</code>).<dd>
<DT>runone()</DT>
<DD>Runs one clustering iteration, starting with computation of data membership (associating data instances to their nearest centroid), followed by re-computation of centroids.</DD>
<DT>run()</DT>
<DD>Runs clustering until the convergence conditions are met. If <code>nstart</code> is greater than one, <code>nstart</code> runs of the clustering algorithm will be executed, returning the clustering with the best (lowest) score.</DD>
</DL>

<p class=section>Attributes</P>
<DL class=attributes>
<DT>k</DT>
<DD>Number of clusters.</DD>
<DT>centroids</DT>
<DD>Current set of centroids.
<DT>scoring</DT>
<DD>Current clustering score.</DD>
<DT>iteration</DT>
<DD>Current clustering iteration.</DD>
<DT>clusters</DT>
<DD>A list of cluster indexes. An i-th element provides an index to a centroid associated with i-th data instance from the input data set.</DD>
</DL>

<p>The following code runs k-means clustering on Iris data set and prints out the cluster indexes for the last 10 data instances:</p>

<p class="header">part of <a href="kmeans-run.py">kmeans-run.py</a> (uses <a href=
"iris.tab">iris.tab</a>)</p>
<xmp class=code>data = orange.ExampleTable("iris")
km = orngClustering.KMeans(data, 3)
print km.clusters[-10:]
</xmp>

<p>The output of this code is:</p>
<xmp class=code>[1, 1, 2, 1, 1, 1, 2, 1, 1, 2]
</xmp>

<p>Invoking a call-back function may be useful when tracing the progress of the clustering. Below is a code that uses an <code>inner_callback</code> to report on the number of instances that have changed the cluster and to report on the clustering score.</p>

<p class="header">part of <a href="kmeans-run-callback.py">kmeans-run-callback.py</a> (uses <a href=
"iris.tab">iris.tab</a>)</p>
<xmp class=code>def callback(km):
    print "Iteration: %d, changes: %d, score: %.4f" % (km.iteration,
        km.nchanges, km.score)
    
data = orange.ExampleTable("iris")
km = orngClustering.KMeans(data, 3, inner_callback = callback)
</xmp>

<p>The convergence on Iris data set is fast:</p>
<xmp class=code>Iteration: 1, changes: 150, score: 10.9555
Iteration: 2, changes: 12, score: 10.3867
Iteration: 3, changes: 2, score: 10.2034
Iteration: 4, changes: 2, score: 10.0699
Iteration: 5, changes: 2, score: 9.9542
Iteration: 6, changes: 1, score: 9.9168
Iteration: 7, changes: 2, score: 9.8624
Iteration: 8, changes: 0, score: 9.8624
</xmp>


<p>Call-back above is used for reporting of the progress, but may as well call a function that plots a selection data projection with corresponding centroid at a given step of the clustering. This is exactly what we did with the following script:</p>

<p class="header">part of <a href="kmeans-trace.py">kmeans-trace.py</a> (uses <a href=
"iris.tab">iris.tab</a>)</p>
<xmp class=code>def plot_scatter(data, km, attx, atty, filename="kmeans-scatter", title=None):
    """plot a data scatter plot with the position of centeroids"""
    pylab.rcParams.update({'font.size': 8, 'figure.figsize': [4,3]})
    x = [float(d[attx]) for d in data]
    y = [float(d[atty]) for d in data]
    colors = ["c", "w", "b"]
    cs = "".join([colors[c] for c in km.clusters])
    pylab.scatter(x, y, c=cs, s=10)
    
    xc = [float(d[attx]) for d in km.centroids]
    yc = [float(d[atty]) for d in km.centroids]
    pylab.scatter(xc, yc, marker="x", c="k", s=200)
    
    pylab.xlabel(attx)
    pylab.ylabel(atty)
    if title:
        pylab.title(title)
    pylab.savefig("%s-%03d.png" % (filename, km.iteration))
    pylab.close()

def in_callback(km):
    print "Iteration: %d, changes: %d, score: %8.6f" % (km.iteration, km.nchanges, km.score)
    plot_scatter(data, km, "petal width", "petal length", title="Iteration %d" % km.iteration)
    
data = orange.ExampleTable("iris")
random.seed(42)
km = orngClustering.KMeans(data, 3, maxiters=10, inner_callback=in_callback)</xmp>

<p>Only the first four scatterplots are shown below. Colors of the data instances indicate the cluster membership. Notice that since the Iris data set includes four attributes, the closest centroid in a particular 2-dimensional projection is not necessary also the centroid of the cluster that the data point belongs to.</p>

<table>
<tr>
<td><img src="kmeans-scatter-001.png"></td>
<td><img src="kmeans-scatter-002.png"></td>
</tr>
<tr>
<td><img src="kmeans-scatter-003.png"></td>
<td><img src="kmeans-scatter-004.png"></td>
</tr>
</table>


<h2>k-Means Utility Functions</h2>

<dl class="attributes">
<dt>kmeans_init_random(data, k, _)</dt>
<dd class="ddfun">A function that can be used for initialization of k-means clustering returns <code>k</code> data instances from the <code>data</code>. This type of initialization is also known as Fory's initialization (Forgy, 1965; He et al., 2004).</dd>
<dt>kmeans_init_diversity(data, k, distfun)</dt>
<dd class="ddfun">Another function that can be used for intializationof k-means clustering. Given the data set, number of clusters and adistance function returns a set of centroids where the first one is a data point being the farthest away from the center of the data, and consequent centroids data points of which the minimal distance to the previous set of centroids is maximal. This type of initialization is almost identical to initialization proposed by Katsavounidis et al. (1994), with the only difference being in the selection of the first centroid (where they use a data instance with the biggest norm).</dd>
<dt>KMeans_init_hierarchicalClustering(n=100)</dt>
<dd class="ddfun">Is actually a class that returns an clustering initialization function that, given the data, k, and a distance function samples <code>n</code> data instances, performs hierarhical clustering, uses it to infer <code>k</code> clusters, and computes a list of cluster-based data centers.</dd>
<dt>data_center(data)</dt>
<dd class="ddfun">Returns a center of the instances in the data set (average across data instances for continuous attributes, most frequent value for discrete attributes).</dd>
<dt>score_distanceToCentroids(kmeans)</dt>
<dd class="ddfun">Returns an average distance of data instances to their associated centroids. <code>kmeans</code> is a k-means clustering object.</dd>

<p>Typically, the choice of seeds has a large impact on the k-means clustering, with better initialization methods yielding a clustering that converges faster and finds more optimal centroids. The following code compares three different initialization methods (random, diversity-based and hierarchical clustering-based) in terms of how fast they converge:</p>

<p class="header">part of <a href="kmeans-cmp-init.py">kmeans-cmp-init.py</a> (uses <a href=
"iris.tab">iris.tab</a>, <a href=
"housing.tab">housing.tab</a>, <a href=
"vehicle.tab">vehicle.tab</a>,)</p>
<xmp class=code>import orange
import orngClustering
import random

data_names = ["iris", "housing", "vehicle"]
data_sets = [orange.ExampleTable(name) for name in data_names]

print "%10s %3s %3s %3s" % ("", "Rnd", "Div", "HC")
for data, name in zip(data_sets, data_names):
    random.seed(42)
    km_random = orngClustering.KMeans(data, centroids = 3)
    km_diversity = orngClustering.KMeans(data, centroids = 3, \
        initialization=orngClustering.kmeans_init_diversity)
    km_hc = orngClustering.KMeans(data, centroids = 3, \
        initialization=orngClustering.KMeans_init_hierarchicalClustering(n=100))
    print "%10s %3d %3d %3d" % (name, km_random.iteration, km_diversity.iteration, km_hc.iteration)
</xmp>

<p>The results show that diversity and clustering-based initialization make k-means converge faster that random selection of seeds (as expected):</p>

<xmp class=code>           Rnd Div  HC
      iris  12   3   4
   housing  14   6   4
   vehicle  11   4   3
</xmp>

<h2>References</h2>

<p>E. Forgy. Cluster analysis of multivariate data: Efficiency versus interpretability of classification. Biometrics, 21(3):768-769, 1965.</p>

<p>J. He, M. Lan, C.-L. Tan, S.-Y. Sung, and H.-B. Low. Initialization of cluster refinement algorithms: A review and comparative study. In Proceedings of International Joint Conference on Neural Networks (IJCNN), pages 297-302, Budapest, Hungary, July 2004.</p>

<p>I. Katsavounidis, C. Jay, and Zhen Zhang. A new initialization technique for generalized Lloyd iteration. Signal Processing Letters, IEEE, 1(10):144-146, 1994.</p>

</body>
</html>

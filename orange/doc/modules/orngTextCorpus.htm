<html><HEAD>
<LINK REL=StyleSheet HREF="../style.css" TYPE="text/css">
</HEAD>
<body>
<h1>orngTextCorpus: Orange Text Analysis Module</h1>

<P>Text Corpus module is the base for textual processing in Orange. It
is used to load textual data and to perform basic preprocessing. The
textual data is loaded from XML file and is outputted as the oranges
<CODE>ExampleTable</CODE>.</P>


<P>Corpus that is stored in XML file, that module loads, must have a
certain structure, which is shown underneath. The tag <CODE>set</CODE>
contains a collection of documents. Each document is enclosed in the
<CODE>document</CODE> tag. Each document may have a
<CODE>categories</CODE> tag which contains <CODE>category</CODE>
tags. The <CODE>category</CODE> tag contains one category of the
document. Tag <CODE>content</CODE> embodies the content of the
document and can contain other tags. Tags within the
<CODE>document</CODE> tag, outside the tag <CODE>content</CODE> are
ignored unless stated different. Tag <CODE>category</CODE> can be
omitted and tags <CODE>category</CODE> can be placed under the tag
<CODE>document</CODE>. XML document can hold other tags whose behavior is
defined by the constructor of the class.</P>

<XMP class="code"><set>
   <document id = "..." ...>
      <categories>
         <category> ... </category> 
         ...
         <category> ... </category>
      </categories>
      <content>
      ...
      </content>
      ...
   </document>
   ...
</set>
</XMP>

<P>Module contains: </P>
<ul>
<li>class <CODE>TextCorpusLoader</CODE> used for loading a XML file</li>
<li>class <CODE>FeatureSelection</CODE> used for selecting features from text</li>
<li>class <CODE>CategoryDocument</CODE>, creates category-feature <CODE>Example Table</CODE></li>
<li>module <CODE>lemmatizer</CODE>, contains two lemmatizers
<ul>
    <li>class <CODE>FSALemmatization</CODE></li>
    <li>class <CODE>NOPLemmatization</CODE></li>
</ul></li>
<li>function <CODE>tokenize</CODE> splits text into tokens (words) </li>
<li>function <CODE>loadWordSet</CODE>, used for loading stop-words </li>
<li>function <CODE>checkFromText</CODE>, cheks whether ExampleTable contains textual data </li>
<li>function <CODE>getCategories</CODE>, retrieves frequencies of categories in data</li>
</ul>

<H2>Class TextCorpusLoader</H2>
<H3>Attributes</H3>
<dl class="attributes">
<dt>data</dt><dd>ExampleTable that contains corpus. One Example in the
ExampleTable represents information about one document from
corpus. </dd>
<dt>lem</dt>
<dd>Reference to an object that has following methods: 
<dl class="attributes">
<dt>getLemmas(word)</dt><dd>returns a list of words that are lemmas of
the <CODE>word</CODE></dd>
<dt>isStopword(word)</dt><dd>return <CODE>true</CODE> if the word is
stop-word, <CODE>false</CODE> otherwise
</dd>
</dl></dd>
</dl>
<H3>Construction</H3>
<P><CODE>TextCorpusLoader</CODE> class is constructed in the following
way:</P>

<XMP class="code">    >>> import orngTextCorpus
    >>> 
    >>> c = orngTextCorpus.TextCorpusLoader('reuters-exchanges.xml')
    >>> exampleTable = c.data
</XMP>

<P>Using this constructor, <CODE>TextCorpusLoader</CODE> object is
created that loads data from XML and stores it in the
<CODE>ExampleTable</CODE>. Function
<CODE>orngTextCorpus.tokenize(text)</CODE> is used for splitting the
content of documents into words. Each word is lemmatized when processed, using 
<CODE>orngTextCorpus.lemmatizer.NOPLemmatization()</CODE> which is
used by default. This lemmatizer does not change the word. </P>

<p>The general form of the <CODE>TextCorpusLoader</CODE>:</P>

<XMP class="code">TextCorpusLoader(  filename,
        tags = {},
        additionalTags = [ ],
        doNotParse = [ ],
        lem = None,
	wordsPerDocRange = (-1, -1),
	charsPerDocRange = (-1, -1)
	)
</XMP>

<dl class="attributes">
<dt>tags</dt>
<dd>
Argument <CODE>tags</CODE> defines the names of the tags desribed
above. This means that tag the name of the tag <CODE>document</CODE>
does not have to be <CODE>document</CODE>, but could be
<CODE>doc</CODE> for instance. Nevertheless XML document must adhere
to the structure described, no matter what the name of the tags
are. If argument <CODE>tags</CODE> is omitted then it defaults to
<XMP class="code">{ 
  "document" : "document", 
  "categories" : "categories", 
  "category" : "category", 
  "content" : "content" 
}
</XMP>
<p>Following statement loads the XML file which has tag
<CODE>doc</CODE> instead of <CODE>document</CODE> and tag
<CODE>data</CODE> instead of <CODE>content</CODE>. Tags
<CODE>categories</CODE> and <CODE>category</CODE> remains with 
the same name. </p>
<XMP class="code">    >>> c = orngTextCorpus.TextCorpusLoader
                   ('reuters-exchanges.xml', 
                    { "document" : "doc", "content" : "data" })
</XMP>
<p>It should be noted here that if the file doesn't have the
<CODE>category</CODE> tag,
everything will work, only the <CODE>ExampleTable</CODE> will not have
the information about the document categories.</P>
</dd>
<dt>additionalTags</dt>
<dd>
When parsing the document all tags, other than <CODE>document,
content, categories</CODE> and <CODE>category</CODE> are ignored. 
<CODE>ExampleTable</CODE> created this way has two attributes
<CODE>category</CODE> and <CODE>meta</CODE>. Attribute
<CODE>category</CODE> contains the information about the categories of
the document. If there are more than one category, they are separated
by point '.', while if category information doesn't exist for the
document it is an empty string ''. Attribute <CODE>meta</CODE>
contains information about the document. This information is retrieved
from the attributes in the <CODE>document</CODE> tag. Content of the document
is stored as meta-attributes in the <CODE>ExampleTable</CODE>
representing the frequencies of the words in the documents.
<P>
By default, <CODE>additionalTags</CODE> is an empty list. In order to
parse other tags than the four listed, they have
to be added to <CODE>additionalTags</CODE>. Additional tags listed are
appended to the domain of the ExampleTable. When a tag listed in
<CODE>additionalTags</CODE> is encountered, its content is stored
under the attribute of the same name.If a document does not have the
particular tag an empty string '' is stored under the attribute.</P>
<P>
If a tag <CODE>TAG</CODE> is nested under the tag <CODE>content</CODE>
and <CODE>TAG</CODE> is not an element of <CODE>doNotParse</CODE>
list, then its content is regarded as the content of 
the tag <CODE>content</CODE> (i.e. tag's name is stripped). If the tag
<CODE>TAG</CODE> is an element of the <CODE>additionalTags</CODE> list
then its content is not added to the <CODE>content</CODE> but it
is stored under the attribute <CODE>TAG</CODE>.
</dd>
<dt>doNotParse</dt>
<dd>Is a list of XML tags that won't be parsed. If omitted every tag will be parsed.</dd>
</dd>
<dt>lem</dt>
<dd>
<P>Argument <CODE>lem</CODE> can receive an object that has following
methods:</P>
<dl class="attributes">
<dt>getLemmas(word)</dt><dd>returns a list of words that are lemmas of
the <CODE>word</CODE></dd>
<dt>isStopword(word)</dt><dd>return <CODE>true</CODE> if the word is
stop-word, <CODE>false</CODE> otherwise</dd>
</dl><br/>
<P>
Accompanied with <CODE>orngTextCorpus</CODE> come two lemmatizers:
</P>
<P>
<ul>
<li>class <CODE>lemmatizer.FSALemmatization('fsa_file')</CODE></li>
<li>class <CODE>lemmatizer.NOPLemmatization()</CODE></li>
</ul></P><BR/>
<P>
They are both a part of the TMT library. <CODE>FSALemmatization</CODE> uses 
lemmatization dictionary to lemmatize words. Creation of the
dictionary is described in in the part where creation of final state
automata is desribed. <CODE>NOPLemmatization</CODE> is used when there
is no dictionary or stemmer available. If 
argument <CODE>lem</CODE>  is omitted, latter lemmatizer will be used.</P><br/>
<P>
Lemmatizers are also used to filter out stop-words. Both lemmatizers
have the attribute <CODE>stopwords</CODE>  which should be filled with
words that will be left out of analysis. If there is no
dictionary available, but there exists a need for filtering out the
stop-words, <CODE>NOPLemmatization</CODE> should be used.</p><br/>
<P>
<XMP class="code">    >>> lem = lemmatizer.FSALemmatization('eng_dict.fsa')
    >>> for word in loadWordSet('eng_stopwords'):
    >>>     lem.stopwords.append(word)       
    >>> c = orngTextCorpus.TextCorpusLoader
    >>>             ('reuters-exchanges.xml', lem = lem)
</XMP></p>
<p>
As an argument to the constructor, file containing lemmatizing
dictionary, written as final state automata, is given. Function
<CODE>loadWordSet</cODe> loads English stopwords from a textual
file. And now, the lemmatizer is ready.</P><br/>
<p>
<i>Note:</i> Any object that has methods <code>getLemmas</code> and
<codE>isStopword</code>  can be used as a lemmatizer.
</P><br/>
</dd>
<dt>wordsPerDocRange</dt>
<dd>Defines a range of words that document needs to have in order to
be loaded. Range is given as a tuple. First element of the tuple
defines minimum number and second element maximum number of words per
document. Putting -1 in place of minimum or maximum defines open
interval. All documents are loaded if ommited. </dd>
<dt>charsPerDocRange</dt>
<dd>
Defines a range of characters that document needs to have in order to
be loaded. Range is given as a tuple. First element of the tuple
defines minimum number and second element maximum number of characters
per document. Putting -1 in place of minimum or maximum defines open
interval. All documents are loaded if ommited. 
</dd>
</dl>
<h2>Class FeatureSelection</h2>
<p>Class is used for dimensionality reduction of textual
data. Dimensionality reduction is performed by calculating
various measures for features in documents and by filtering them out
using Orange filters.</p>
<h3>attributes</h3>
<dl class="attributes">
<dt>measures
</dt><dd>Dictionary containg available measures for feature
selection.</dd>
<dt>dataInput</dt>
<dd>ExampleTable that contains corpus given as an argument to the
constructor.</dd>
<dt>data</dt>
<dd>ExampleTable created by the class. It contains examples whose
names are meta-attributes of <CODE>dataInput</CODE>. Attributes are
names of the feature selection method. </dd>
</dl>
<h3>Methods</h3>
<dl class="attributes">
<dt>getFeatureMeasures</dt><dd>Returns attribute <CODE>data</CODE>.</dd>
<dt>selectFeatures(filter = None, list = None)</dt><dd>Returns
ExampleTable whose examples are equal to the examples of
the<CODE>dataInput</CODE>, but with removed meta-attributes. Method
uses <CODE>filter</CODE>, if defined. Filter is created on the
<CODE>data</CODE>. If there is no filter, list is used for fitering
out meta-attributes. List should contain names of meta-attributes to
remove. </dd></dl>
<h3>Construction</h3>
<XMP class="code">FeatureSelection(  dataInput,
             userMeasures = [ ]
	     )
</XMP>
<dl class="attributes">
<dt>dataInput</dt><dd> input ExampleTable, contains textaul
corpus</dd>
<dt>userMeasures</dt><dd>currently not implemented</dd>
</dl>

<h2>Class CategoryDocument</h2>
<P>Class is used for dimensionality reduction of textual
data. Dimensionality reduction is performed by calculating various
measures for features in documents and by filtering them out using
Orange filters. </p>
<h3>Attributes</h3>
<dl class="attributes">
<dt>data</dt><dd>input data</dd>
<dt>dataCD</dt><dd>output data, ExampleTable containing category-word matrix</dd>
</dl>
<h3>Construction</h3>
<dl class="attributes">
<dt>CategoryDocument(data)</dt>
<dd><CODE>data</CODe> contains corpus</dd>
</dl>
<h2>Module lemmatizer</h2>
<p>Module is described under TextCorpusLoader section.</p>
<h2>Functions</h2>
<dl class="attributes">
<dt>tokenize( text )</dt>
<dd>Function splits text into lexical units, tokens. White spaces are
used for separation.</dd>
<dt>loadWordSet( fileName )</dt>
<dd>Function loads words from the file <CODE>fileName</CODE>. Each
line in a file contains exactly one word. Empty lines are
ignored. Line can contain a character ';'. Everything right to ';' is
ignored. White spaces to the left and to the right of the word are
stripped.</dd>
<dt>checkFromText( ExampleTable )</dt>
<dd>Function checks whether the <CODE>ExampleTable</CODE>  contains textual data. It
includes probability and is not 100% correct. Returns boolean
value.</dd>
<dt>getCategories( ExampleTable )</dt>
<dd>Returns dictionary whose keys are categories found in <CODE>ExampleTable</CODE>
and values are number of documents classified under the category.</dd>
</dl>

<h2>Creating final state automata</h2>
<p>One of the steps in preprocessing textual data is lemmatization and
removal of stop words. Stop words are common words, or words
considered to be irrelevant, they are left out when processing the
data. Stop words include conjunctions, prepositions, and articles such
as and, to and a. Lemmatization is a form of linguistic processing
that determines the lemma for each word form that occurs in text. The
lemma of a word encompasses its base form plus inflected forms that
share the same part of speech. For example, the lemma for go
encompasses go, goes, went, gone, and going. In case when there is no
dictionary available stemmers can be used instead of lemmatizing
words. A stemmer is a computer program or algorithm which determines a
stem form of a given inflected word form. The stem need not be
identical to the morphological root of the word; it is usually
sufficient that related words map to the same stem, even if this stem
is not in itself a valid root. 
</p>

<p>Stemmers are fairly easy to build for English or Spanish because of
the morphological simplicity of the languages. As the morphology,
orthography, and character encoding of the target language becomes
more complex, it becomes more difficult to create a good stemmer for
the language. For example, it would be very hard to create an
efficient stemmer for Croatian language because of its morphological
complexity. One of the most known English stemmer was written by
Martin Porter in 1980. This stemmer became very widely used, and
became the de-facto standard algorithm used for English stemming.</P>

<p>Module <cODE>orngTextCorpus</code> provides lemmatization, stemming using
Porter¡Çs algorithm and removal of stop words.</P>

<p>Lemmatization can be done using any language if there is a
dictionary available. Dictionary is stored in a compressed final state
automata format, created by the algorithm of Jan Daciuk. Using his
toolkit <a
href="http://www.eti.pg.gda.pl/katedry/kiw/pracownicy/Jan.Daciuk/personal/fsa.html#FSApack">fsa_build</a>
dictionary is stored in fsa file. Dictionary,
prior to storing in fsa, has to be stored in a textual file whose
every line consist of 3 columns: inflected word, canonical form,
annotations and it should be sorted. More information can be found in
fsa_build manual.
</p>
</body> </html>
